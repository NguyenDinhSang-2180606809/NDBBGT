# -*- coding: utf-8 -*-
"""NhanDienBBGT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-B_f3so2dMq4UI38LRrrL_mVRqBhsleI
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
import os
import shutil
import yaml

from google.colab import drive
drive.mount('/content/drive')
from tensorflow.keras.preprocessing.image import ImageDataGenerator

!unzip "/content/drive/MyDrive/DeepLearning/dataset/bbgt.zip" -d "/content/drive/MyDrive/DeepLearning/dataset"

# STEP 1: TAI DU LIEU DATASET
dataset_path = "/content/drive/MyDrive/DeepLearning/dataset"
# Äá»c file data.yaml
yaml_path = os.path.join(dataset_path, "data.yaml")
with open(yaml_path, "r") as f:
    data = yaml.safe_load(f)
    classes = data.get("names", [])  # Láº¥y danh sÃ¡ch nhÃ£n

print("âœ… Danh sÃ¡ch class:", classes)

dataset_path = "/content/drive/MyDrive/DeepLearning/dataset"

for split in ["train", "valid", "test"]:
    img_dir = os.path.join(dataset_path, split, "images")
    label_dir = os.path.join(dataset_path, split, "labels")

    if not os.path.exists(img_dir):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c {img_dir}")
    else:
        print(f"ğŸ“‚ {split}/images: {len(os.listdir(img_dir))} áº£nh")

    if not os.path.exists(label_dir):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c {label_dir}")
    else:
        print(f"ğŸ“„ {split}/labels: {len(os.listdir(label_dir))} file nhÃ£n")

dataset_path = "/content/drive/MyDrive/DeepLearning/dataset"
output_path = "/content/drive/MyDrive/DeepLearning/dataset_classification"
os.makedirs(output_path, exist_ok=True)

# Load danh sÃ¡ch nhÃ£n tá»« data.yaml
yaml_path = os.path.join(dataset_path, "data.yaml")
with open(yaml_path, "r") as f:
    data = yaml.safe_load(f)
    classes = data.get("names", [])

def convert_dataset(split):
    img_dir = os.path.join(dataset_path, split, "images")
    label_dir = os.path.join(dataset_path, split, "labels")

    # Kiá»ƒm tra náº¿u thÆ° má»¥c khÃ´ng tá»“n táº¡i
    if not os.path.exists(img_dir) or not os.path.exists(label_dir):
        print(f"âš ï¸ Bá» qua {split} vÃ¬ thiáº¿u thÆ° má»¥c áº£nh hoáº·c nhÃ£n.")
        return

    print(f"ğŸ”„ Chuyá»ƒn Ä‘á»•i {split}...")

    for label_file in os.listdir(label_dir):
        label_path = os.path.join(label_dir, label_file)
        img_name = label_file.replace(".txt", ".jpg")  # Äá»•i Ä‘uÃ´i file nhÃ£n sang áº£nh
        img_path = os.path.join(img_dir, img_name)

        # Kiá»ƒm tra náº¿u áº£nh khÃ´ng tá»“n táº¡i
        if not os.path.exists(img_path):
            print(f"âŒ KhÃ´ng tÃ¬m tháº¥y áº£nh: {img_path}, bá» qua.")
            continue

        # Äá»c file nhÃ£n
        with open(label_path, "r") as f:
            lines = f.readlines()

        # Náº¿u file nhÃ£n rá»—ng, bá» qua
        if len(lines) == 0:
            print(f"âš ï¸ File {label_path} trá»‘ng, bá» qua.")
            continue

        # Láº¥y class ID tá»« dÃ²ng Ä‘áº§u tiÃªn
        class_id = int(lines[0].split()[0])

        # Kiá»ƒm tra class ID há»£p lá»‡
        if class_id >= len(classes):
            print(f"âŒ Lá»—i: Class ID {class_id} vÆ°á»£t quÃ¡ sá»‘ lÆ°á»£ng lá»›p.")
            continue

        class_name = classes[class_id]

        # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
        class_path = os.path.join(output_path, split, class_name)
        os.makedirs(class_path, exist_ok=True)

        # Copy áº£nh vÃ o thÆ° má»¥c class
        shutil.copy(img_path, os.path.join(class_path, img_name))

    print(f"âœ… HoÃ n thÃ nh chuyá»ƒn Ä‘á»•i {split}!")

# Chuyá»ƒn Ä‘á»•i cáº£ 3 táº­p dá»¯ liá»‡u
convert_dataset("train")
convert_dataset("valid")
convert_dataset("test")

print("âœ… Dataset Ä‘Ã£ chuyá»ƒn Ä‘á»•i xong!")

output_path = "/content/drive/MyDrive/DeepLearning/dataset_classification"

for split in ["train", "valid", "test"]:
    split_path = os.path.join(output_path, split)
    if os.path.exists(split_path):
        num_classes = len(os.listdir(split_path))
        total_images = sum([len(os.listdir(os.path.join(split_path, class_name))) for class_name in os.listdir(split_path)])
        print(f"ğŸ“‚ {split}: {num_classes} lá»›p, {total_images} áº£nh")
    else:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c {split}")

# Äá»‹nh nghÄ©a Ä‘Æ°á»ng dáº«n
train_dir = "/content/drive/MyDrive/DeepLearning/dataset_classification/train"
valid_dir = "/content/drive/MyDrive/DeepLearning/dataset_classification/valid"
test_dir = "/content/drive/MyDrive/DeepLearning/dataset_classification/test"

# KÃ­ch thÆ°á»›c áº£nh Ä‘áº§u vÃ o
IMG_SIZE = (128, 128)  # Resize áº£nh vá» 128x128
BATCH_SIZE = 32

# Táº¡o ImageDataGenerator Ä‘á»ƒ load áº£nh tá»« thÆ° má»¥c
train_datagen = ImageDataGenerator(rescale=1./255)
valid_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Load dá»¯ liá»‡u tá»« thÆ° má»¥c
train_data = train_datagen.flow_from_directory(
    train_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False
)

valid_data = valid_datagen.flow_from_directory(
    valid_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False
)

test_data = test_datagen.flow_from_directory(
    test_dir, target_size=IMG_SIZE, batch_size=BATCH_SIZE, class_mode='categorical', shuffle=False
)

# Láº¥y toÃ n bá»™ dá»¯ liá»‡u tá»« generator vÃ  chuyá»ƒn thÃ nh numpy array
X_train, y_train = next(train_data)
X_test, y_test = next(test_data)

# In thÃ´ng tin dá»¯ liá»‡u
print(f"âœ… X_train shape: {X_train.shape}, y_train shape: {y_train.shape}")
print(f"âœ… X_test shape: {X_test.shape}, y_test shape: {y_test.shape}")

# Láº¥y toÃ n bá»™ dá»¯ liá»‡u train
X_train, y_train = [], []
for _ in range(len(train_data)):
    X, y = next(train_data)  # DÃ¹ng next() thay vÃ¬ .next()
    X_train.append(X)
    y_train.append(y)

# Chuyá»ƒn thÃ nh numpy array
X_train = np.vstack(X_train)
y_train = np.vstack(y_train)
print(f"âœ… Dá»¯ liá»‡u train Ä‘áº§y Ä‘á»§: {X_train.shape}, {y_train.shape}")

# Láº¥y toÃ n bá»™ dá»¯ liá»‡u test
X_test, y_test = [], []
for _ in range(len(test_data)):
    X, y = next(test_data)  # DÃ¹ng next() thay vÃ¬ .next()
    X_test.append(X)
    y_test.append(y)

# Chuyá»ƒn thÃ nh numpy array
X_test = np.vstack(X_test)
y_test = np.vstack(y_test)
print(f"âœ… Dá»¯ liá»‡u test Ä‘áº§y Ä‘á»§: {X_test.shape}, {y_test.shape}")

#STEP 2: HIEN THI MOT SO ANH VI DU
idx = 1500

plt.figure(figsize=(6, 3))
plt.imshow(X_train[idx])  # Hiá»ƒn thá»‹ áº£nh mÃ u
plt.title(f"Biá»ƒn bÃ¡o: {np.argmax(y_train[idx])}", fontsize=12)
  # Chuyá»ƒn tá»« one-hot vá» sá»‘ nguyÃªn
plt.axis('off')  # áº¨n trá»¥c tá»a Ä‘á»™
plt.show()



# Hiá»ƒn thá»‹ thÃªm 5 vÃ­ dá»¥ khÃ¡c
plt.figure(figsize=(12, 4))
num_images = 5
for i in range(num_images):
    idx = np.random.randint(1, 6049)  # Chá»n ngáº«u nhiÃªn chá»‰ sá»‘ tá»« 1 Ä‘áº¿n 6000
    plt.subplot(1, num_images, i+1)
    plt.imshow(X_train[idx])
    plt.title(f"Biá»ƒn bÃ¡o: {np.argmax(y_train[idx])}", fontsize=12)
    plt.axis('off')
plt.tight_layout()
plt.show()

# STEP 1: XÃ‚Y Dá»°NG MÃ” HÃŒNH CNN (Sá»¬A Láº I CHO 7 Lá»šP)
NUM_CLASSES = 7  # Sá»‘ lá»›p thá»±c táº¿
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(NUM_CLASSES, activation='softmax')  # Sá»­a tá»« 2 thÃ nh 7 lá»›p
])

# BiÃªn dá»‹ch mÃ´ hÃ¬nh
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# STEP 2: HUáº¤N LUYá»†N MÃ” HÃŒNH
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=32,
    validation_data=(X_test, y_test)
)

# STEP 3: ÄÃNH GIÃ MÃ” HÃŒNH
loss, accuracy = model.evaluate(X_test, y_test)
print(f"âœ… Test accuracy: {accuracy*100:.2f}%")

# STEP 4: Váº¼ Äá»’ THá»Š HIá»†U SUáº¤T
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

# STEP 5: Dá»° ÄOÃN VÃ€ HIá»‚N THá»Š Káº¾T QUáº¢ MáºªU (CHá»ŒN Tá»ª X_test)
indices = np.random.randint(0, 299, 5)  # Chá»n ngáº«u nhiÃªn 5 chá»‰ sá»‘ tá»« 1 Ä‘áº¿n 317
X_sample = X_test[indices]
y_sample_true = y_test[indices]
y_sample_pred = model.predict(X_sample)

plt.figure(figsize=(15, 5))
for i in range(5):
    plt.subplot(1, 5, i+1)
    plt.imshow(X_sample[i])
    true_label = np.argmax(y_sample_true[i])
    pred_label = np.argmax(y_sample_pred[i])
    plt.title(f"True: {true_label}\nPred: {pred_label}")
    plt.axis('off')
plt.tight_layout()
plt.show()

# STEP 6: LÆ¯U MÃ” HÃŒNH
model.save('/content/drive/MyDrive/DeepLearning/traffic_sign_cnn_7classes.h5')
print("âœ… MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: /content/drive/MyDrive/DeepLearning/traffic_sign_cnn_7classes.h5")